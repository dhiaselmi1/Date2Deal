import json
import os
from datetime import datetime
from typing import Dict, List, Optional
import re
import requests
import glob  # Ensure glob is imported for pattern matching


class YouTubeAnalysisAgent:
    GEMINI_API_KEY = "AIzaSyAcUgGfaSrzt7X11r-ree0X70aOK7VLj2g"

    def __init__(self):
        if not self.GEMINI_API_KEY or self.GEMINI_API_KEY == "VOTRE_CL√â_API_GEMINI_ICI":
            raise ValueError(
                "Cl√© API Gemini requise. Veuillez remplacer 'VOTRE_CL√â_API_GEMINI_ICI' par votre cl√© r√©elle.")
        print("Agent d'analyse YouTube initialis√© avec la cl√© API Gemini.")

    def load_latest_json_data(self) -> Optional[Dict]:
        """
        Charge le fichier JSON le plus r√©cent g√©n√©r√© par youtube_search.py ou le script pr√©c√©dent.
        Recherche les fichiers dans le r√©pertoire du script agent_youtube.py.

        Returns:
            Dictionnaire contenant les donn√©es extraites ou None si erreur
        """
        try:
            print("üîç Recherche du fichier JSON le plus r√©cent...")

            # Get the directory of the current script (agent_youtube.py)
            script_dir = os.path.dirname(os.path.abspath(__file__))

            # Search for files using glob with the script's directory
            search_pattern_old = os.path.join(script_dir, 'youtube_results_*.json')
            search_pattern_new = os.path.join(script_dir, 'youtube_search_*.json')

            json_files_old_format = glob.glob(search_pattern_old)
            json_files_new_format = glob.glob(search_pattern_new)

            all_json_files = sorted(json_files_old_format + json_files_new_format, key=os.path.getmtime, reverse=True)

            if not all_json_files:
                print("‚ùå Aucun fichier JSON de r√©sultats YouTube trouv√© dans le r√©pertoire du script.")
                print(f"üí° V√©rifi√© dans : {script_dir}")
                print("üí° Assurez-vous d'avoir ex√©cut√© le script de recherche et de sauvegarde d'abord.")
                return None

            latest_file = all_json_files[0]
            print(f"üìÑ Fichier le plus r√©cent trouv√©: {latest_file}")

            # Read JSON data
            with open(latest_file, 'r', encoding='utf-8') as f:
                raw_data_from_file = json.load(f)

            processed_data = {}
            # Detect JSON file format
            if isinstance(raw_data_from_file, dict) and 'videos' in raw_data_from_file:
                processed_data['search_query'] = raw_data_from_file.get('search_query', 'Inconnu')
                processed_data['search_date'] = raw_data_from_file.get('search_date', 'Inconnue')
                processed_data['videos'] = raw_data_from_file.get('videos', [])
            elif isinstance(raw_data_from_file, list):
                # This is the old format (direct list of videos)
                processed_data['videos'] = raw_data_from_file
                # Deduce search_query and search_date from filename for old format
                search_query_from_filename = ""
                if latest_file.startswith('youtube_results_'):
                    parts = os.path.basename(latest_file).replace('youtube_results_', '').replace('.json', '').split(
                        '_')
                    if len(parts) > 2 and re.match(r'\d{8}', parts[-2]) and re.match(r'\d{6}', parts[-1]):
                        search_query_from_filename = ' '.join(parts[:-2])
                    else:
                        search_query_from_filename = ' '.join(parts)
                processed_data['search_query'] = search_query_from_filename.replace('_', ' ').strip()
                processed_data['search_date'] = datetime.fromtimestamp(os.path.getctime(latest_file)).strftime(
                    "%Y-%m-%d %H:%M:%S")
            else:
                print(f"‚ùå Format de fichier JSON inattendu: {latest_file}")
                return None

            print(f"‚úÖ Donn√©es charg√©es: {len(processed_data.get('videos', []))} vid√©os")
            print(f"üìÖ Date de recherche: {processed_data.get('search_date', 'Inconnue')}")
            print(f"üîç Terme recherch√© (d√©duit du fichier): '{processed_data.get('search_query', 'Inconnu')}'")

            return processed_data

        except json.JSONDecodeError as e:
            print(f"‚ùå Erreur lors du d√©codage JSON: {e}")
            return None
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
            return None

    def clean_and_prepare_data(self, raw_data: Dict) -> str:
        """
        Nettoie et pr√©pare les donn√©es pour l'analyse LLM

        Args:
            raw_data: Donn√©es brutes du scraping YouTube

        Returns:
            Texte format√© pour le LLM
        """
        if not raw_data or not raw_data.get('videos'):
            return "Aucune donn√©e vid√©o disponible."

        print("üßπ Nettoyage et pr√©paration des donn√©es...")

        formatted_text = f"RECHERCHE YOUTUBE: {raw_data['search_query']}\n"
        formatted_text += f"DATE DE RECHERCHE: {raw_data['search_date']}\n"
        formatted_text += f"NOMBRE DE VID√âOS ANALYS√âES: {len(raw_data['videos'])}\n\n"

        videos_with_subtitles = sum(1 for video in raw_data['videos'] if
                                    isinstance(video, dict) and video.get('subtitles_available', False) and video.get(
                                        'full_speech'))
        formatted_text += f"VID√âOS AVEC SOUS-TITRES DISPONIBLES: {videos_with_subtitles}/{len(raw_data['videos'])}\n\n"

        for i, video in enumerate(raw_data['videos'], 1):
            if not isinstance(video, dict):
                print(
                    f"‚ö†Ô∏è Avertissement: L'√©l√©ment {i} dans la liste des vid√©os n'est pas un dictionnaire et sera ignor√©: {video}")
                continue

            formatted_text += f"=== VID√âO {i} ===\n"
            formatted_text += f"Titre: {video.get('title', 'Titre non disponible')}\n"
            formatted_text += f"ID Vid√©o: {video.get('video_id', 'ID non disponible')}\n"
            formatted_text += f"URL: {video.get('url', 'URL non disponible')}\n"

            if video.get('subtitles_available', False) and video.get('full_speech'):
                clean_speech = self._clean_subtitle_text(video['full_speech'])
                formatted_text += f"Contenu (sous-titres): {clean_speech}\n"
                formatted_text += f"Longueur du contenu: {len(clean_speech)} caract√®res\n"
            else:
                error_msg = video.get('error', 'Raison inconnue')
                formatted_text += f"Contenu: Sous-titres non disponibles ({error_msg})\n"

            formatted_text += "\n" + "-" * 80 + "\n\n"

        print(f"‚úÖ Donn√©es pr√©par√©es: {len(formatted_text)} caract√®res au total")
        return formatted_text

    def _clean_subtitle_text(self, text: str) -> str:
        if not text:
            return ""

        text = re.sub(r'&#\d+;', '', text)
        text = re.sub(r'&[a-zA-Z]+;', '', text)
        text = re.sub(r'&[a-zA-Z0-9#]+;', '', text)

        text = text.replace('&#39;', "'").replace('&quot;', '"').replace('&amp;', '&')

        text = re.sub(r'\b(\w+)(\s+\1\b)+', r'\1', text)

        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\t+', ' ', text)

        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)

        if len(text) > 12000:
            text = text[:12000] + "... [TEXTE TRONQU√â]"

        return text.strip()

    def analyze_with_llm(self, prepared_data: str, person_focus: str = None) -> str:
        try:
            print("ü§ñ Analyse en cours avec le LLM (Gemini)...")

            focus_text = person_focus if person_focus else "la personnalit√© ou le sujet principal"

            system_instruction = f"""Tu es un analyste expert sp√©cialis√© dans l'analyse de contenu YouTube et le profiling de personnalit√©s. 
                Ta mission est d'analyser les transcriptions de vid√©os pour cr√©er un profil complet, d√©taill√© et professionnel de {focus_text}.

                INSTRUCTIONS D√âTAILL√âES:
                1. Analyse TOUS les contenus disponibles avec attention.
                2. Extrait TOUTES les informations pertinentes sur {focus_text}.
                3. Distingue clairement les FAITS des OPINIONS.
                4. Identifie les patterns de comportement et de communication.
                5. Note les √©volutions dans le temps si d√©tect√©es.
                6. Cr√©e une synth√®se coh√©rente, compl√®te et bien structur√©e.
                7. Utilise des citations directes quand c'est pertinent.
                8. Indique le niveau de confiance pour chaque information.

                FORMAT DE R√âPONSE OBLIGATOIRE:
                # PROFIL COMPLET DE {focus_text.upper()}

                ## üéØ R√âSUM√â EX√âCUTIF
                [Synth√®se compl√®te en 3-4 phrases qui capture l'essence de la personne/sujet]

                ## üìã INFORMATIONS BIOGRAPHIQUES
                - **Nom complet:** [Si mentionn√©]
                - **√Çge/Date de naissance:** [Si mentionn√©]
                - **Origine g√©ographique:** [Si mentionn√©]
                - **Formation:** [Si mentionn√©]
                - **Parcours personnel:** [√âl√©ments de vie personnelle mentionn√©s]

                ## üíº ACTIVIT√âS PROFESSIONNELLES
                - **M√©tier principal:** [Profession actuelle]
                - **Projets en cours:** [Projets mentionn√©s]
                - **Collaborations:** [Partenaires, √©quipes]
                - **R√©alisations:** [Succ√®s, accomplissements]
                - **Objectifs futurs:** [Projets √† venir mentionn√©s]

                ## üé≠ PERSONNALIT√â ET STYLE
                - **Traits de caract√®re:** [Personnalit√© observ√©e]
                - **Style de communication:** [Fa√ßon de s'exprimer]
                - **Humour et ton:** [Type d'humour, ambiance]
                - **Valeurs exprim√©es:** [Principes, valeurs mentionn√©es]
                - **Rapport au public:** [Relation avec l'audience]

                ## üî• CENTRES D'INT√âR√äT ET EXPERTISE
                - **Sujets de pr√©dilection:** [Th√®mes favoris r√©currents]
                - **Domaines d'expertise:** [Comp√©tences d√©montr√©es]
                - **Passions personnelles:** [Hobbies, int√©r√™ts]
                - **Tendances r√©currentes:** [Sujets qui reviennent souvent]

                ## üí≠ OPINIONS ET POSITIONS
                - **Prises de position:** [Opinions clairement exprim√©es]
                - **Controverses:** [Sujets sensibles abord√©s]
                - **√âvolutions d'opinion:** [Changements d√©tect√©s]
                - **Nuances:** [Subtilit√©s dans les positions]

                ## üé¨ ANALYSE DU CONTENU VID√âO
                - **Nombre de vid√©os analys√©es:** [Statistique]
                - **Th√®mes principaux:** [Sujets les plus abord√©s]
                - **√âvolution du contenu:** [Changements dans le temps]
                - **Qualit√© des informations:** [Fiabilit√© des sous-titres]
                - **Citations marquantes:** [Phrases importantes extraites]

                ## ‚ö†Ô∏è NOTES M√âTHODOLOGIQUES
                - **Sources d'information:** [Vid√©os analys√©es]
                - **Limites de l'analyse:** [Ce qui manque]
                - **Niveau de confiance:** [Fiabilit√© des informations]
                - **Recommandations:** [Pour une analyse plus pouss√©e]
                """

            user_prompt_full = f"""{system_instruction}

Voici les donn√©es extraites des vid√©os YouTube √† analyser en d√©tail:

{prepared_data}

Analyse ces contenus de mani√®re EXHAUSTIVE et fournis une synth√®se COMPL√àTE selon les instructions. N'omets aucune information importante et sois aussi d√©taill√© que possible."""

            payload = {
                "contents": [
                    {
                        "role": "user",
                        "parts": [{"text": user_prompt_full}]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.2,
                    "topP": 0.9,
                    "maxOutputTokens": 3000
                }
            }

            gemini_api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={self.GEMINI_API_KEY}"

            response = requests.post(gemini_api_url, headers={'Content-Type': 'application/json'}, json=payload)
            response.raise_for_status()

            gemini_response = response.json()

            analysis = ""
            if gemini_response and gemini_response.get('candidates'):
                for candidate in gemini_response['candidates']:
                    if candidate.get('content') and candidate['content'].get('parts'):
                        for part in candidate['content']['parts']:
                            if part.get('text'):
                                analysis += part['text']

            if not analysis:
                print(f"‚ö†Ô∏è La r√©ponse de Gemini ne contient pas de texte. R√©ponse brute: {gemini_response}")
                return "Erreur: Le mod√®le Gemini n'a pas retourn√© de texte analys√©."

            print("‚úÖ Analyse LLM (Gemini) termin√©e avec succ√®s")
            return analysis

        except requests.exceptions.HTTPError as errh:
            print(f"‚ùå Erreur HTTP lors de l'appel √† l'API Gemini : {errh}")
            print(f"D√©tails de la r√©ponse : {response.text}")
            return f"Erreur lors de l'analyse: Erreur HTTP - {errh}"
        except requests.exceptions.RequestException as err:
            print(f"‚ùå Erreur de Requ√™te lors de l'appel √† l'API Gemini : {err}")
            return f"Erreur lors de l'analyse: Erreur de Requ√™te - {err}"
        except json.JSONDecodeError:
            print(f"‚ùå Erreur : Impossible de d√©coder la r√©ponse JSON de l'API Gemini. R√©ponse brute : {response.text}")
            return "Erreur lors de l'analyse: Erreur de d√©codage JSON de la r√©ponse Gemini."
        except Exception as e:
            print(f"‚ùå Erreur inattendue lors de l'analyse LLM : {e}")
            return f"Erreur lors de l'analyse: {str(e)}"

    def save_analysis(self, analysis: str, original_search_term: str) -> str:
        """
        Sauvegarde l'analyse dans un fichier Markdown
        Sauvegarde le fichier dans le r√©pertoire du script agent_youtube.py.

        Args:
            analysis: Texte de l'analyse
            original_search_term: Terme de recherche original

        Returns:
            Nom du fichier sauvegard√©
        """
        script_dir = os.path.dirname(os.path.abspath(__file__))
        safe_term = "".join(c for c in original_search_term if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_term = safe_term.replace(' ', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"youtube_analysis_{safe_term}_{timestamp}.md"

        full_file_path = os.path.join(script_dir, filename)

        try:
            with open(full_file_path, 'w', encoding='utf-8') as f:
                f.write(f"# Analyse YouTube Compl√®te - {original_search_term}\n\n")
                f.write(f"**Date de g√©n√©ration:** {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')}\n\n")
                f.write(f"**Terme de recherche original:** {original_search_term}\n\n")
                f.write("---\n\n")
                f.write(analysis)
                f.write("\n\n---\n\n")
                f.write("*Analyse g√©n√©r√©e automatiquement par YouTubeAnalysisAgent (avec Gemini)*\n")

            print(f"üíæ Analyse sauvegard√©e dans: {full_file_path}")
            return full_file_path

        except Exception as e:
            print(f"‚ùå Erreur lors de la sauvegarde: {e}")
            return ""

    def run_complete_analysis(self, person_focus: str = None) -> Optional[str]:
        """
        Ex√©cute l'analyse compl√®te: chargement -> nettoyage -> analyse LLM -> sauvegarde

        Args:
            person_focus: Nom de la personne √† analyser (optionnel)

        Returns:
            Nom du fichier d'analyse g√©n√©r√© ou None si erreur
        """
        print("üöÄ D√©marrage de l'analyse compl√®te des donn√©es YouTube")
        print("=" * 70)

        # 1. Chargement des donn√©es JSON
        raw_data = self.load_latest_json_data()
        if not raw_data:
            print("‚ùå Impossible de charger les donn√©es YouTube")
            return None

        search_term = raw_data.get('search_query', 'Recherche inconnue')
        print(f"‚úÖ Donn√©es charg√©es pour la recherche: '{search_term}'")

        # 2. Nettoyage et pr√©paration des donn√©es
        prepared_data = self.clean_and_prepare_data(raw_data)
        if not prepared_data or "Aucune donn√©e vid√©o disponible." in prepared_data:
            print("‚ùå Aucune donn√©e utilisable apr√®s nettoyage ou aucun sous-titre disponible pour l'analyse.")
            return None

        print("‚úÖ Donn√©es nettoy√©es et pr√©par√©es pour l'analyse")

        # 3. Analyse avec LLM
        effective_person_focus = person_focus if person_focus else search_term
        print(f"üéØ Focus de l'analyse: {effective_person_focus}")
        analysis = self.analyze_with_llm(prepared_data, effective_person_focus)

        if "Erreur lors de l'analyse" in analysis:
            print("‚ùå Erreur lors de l'analyse LLM")
            return None

        # 4. Sauvegarde
        filename = self.save_analysis(analysis, search_term)
        if not filename:
            print("‚ùå Erreur lors de la sauvegarde")
            return None

        print("=" * 70)
        print("üéâ ANALYSE TERMIN√âE AVEC SUCC√àS!")
        print(f"üìÑ Fichier g√©n√©r√©: {filename}")
        print(f"üìä Longueur de l'analyse: {len(analysis)} caract√®res")

        print("\n" + "=" * 70)
        print("üìã APER√áU DE L'ANALYSE:")
        print("=" * 70)


        return filename


def main():
    """
    Fonction principale pour utiliser l'agent
    """
    print("üé¨ AGENT D'ANALYSE YOUTUBE - DONN√âES EXISTANTES")
    print("=" * 60)
    print("Ce programme analyse le fichier JSON le plus r√©cent")
    print("g√©n√©r√© par le script de recherche YouTube.")
    print("=" * 60)

    try:
        agent = YouTubeAnalysisAgent()

        raw_data = agent.load_latest_json_data()
        if not raw_data:
            print("‚ùå Impossible de charger les donn√©es YouTube. L'analyse ne peut pas continuer.")
            return

        person_focus = raw_data.get('search_query', 'Analyse g√©n√©rale')
        print(f"\nüë§ Focus de l'analyse d√©duit du fichier: '{person_focus}'")

        print(f"\nüîÑ Lancement de l'analyse...")
        print(f"üìä Analyse du contenu avec focus sur: {person_focus}")

        result = agent.run_complete_analysis(person_focus)

        if result:
            print(f"\n‚úÖ Analyse termin√©e avec succ√®s!")
            print(f"üìÅ Consultez le fichier: {result}")
            print("\nüí° Le fichier contient une analyse compl√®te et d√©taill√©e.")
        else:
            print("\n‚ùå L'analyse a √©chou√©. V√©rifiez les logs ci-dessus.")
            print("üí° Assurez-vous qu'un fichier JSON de r√©sultats existe (ex√©cutez le script de recherche d'abord).")

    except ValueError as e:
        print(f"‚ùå Erreur de configuration: {e}")
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {e}")


if __name__ == "__main__":
    main()
